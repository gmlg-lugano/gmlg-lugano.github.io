<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Graph Machine Learning Group</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/particles.css" />
		<link rel="stylesheet" href="assets/css/collapsible.css" />
		<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125823175-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-125823175-2');
		</script>
	</head>
	<body class="is-preload">
		<div id="particles-js"></div>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.svg" alt="" /></span>
						<h1>Graph Machine Learning Group</h1>
						<p>Graph Machine Learning &bull; Non-stationary environments &bull; Reinforcement Learning &bull; Dynamics of RNNs</p>
						<p><span class="icon solid fa-map-marker-alt"></span>&nbsp;&nbsp;<a href="https://usi.ch">Università della Svizzera italiana</a></p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#about" class="active">About</a></li>
							<li><a href="#group">People</a></li>
							<li><a href="#publications">Publications</a></li>
							<li><a href="#oss">Open Source</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="about" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About GMLG</h2>
										</header>
										<p>We are a research team part of the <a href="http://www.alari.ch/">Advanced Learning and Research Institute (ALaRI)</a> at <a href="https://usi.ch/">Università della Svizzera italiana</a>, Lugano, Switzerland. The group is led by Prof. C. Alippi.<br/>
										Our research focuses on machine learning on graphs, non-stationary and evolving environments, and dynamical systems. Applications cover neuroscience, power grids, chemistry, dynamical systems, among many others.</p>
									</div>
									<span class="image"><img src="images/black-building.jpg" alt="" /></span>
								</div>
							</section>

						<!-- Group -->
							<section id="group" class="main special">
								<header class="major">
									<h2>People</h2>
								</header>
								<ul class="people">
									<li style="width: 32em !important;">
										<span class="image"><img src="images/CA.jpg" alt="" /></span>
										<h3><b><a href="http://home.deib.polimi.it/alippi/" class="name">Cesare Alippi</a></b></h3>
										<p>Degree in Electronic Engineering in 1990 and PhD in 1995 from Politecnico di Milano (Italy). Currently, he is a Professor with the Politecnico di Milano (Italy) and Università della Svizzera italiana (Switzerland). 
										His research interests are graph-based learning, learning in non-stationary environments, lifelong learning, intelligence in embedded, cyber-physical systems and the Internet-of-Things.</p>
									</li>
								</ul>
								<ul class="people">
									<li>
										<span class="image"><img src="images/DZ.jpg" alt="" /></span>
										<h3><b><a href="https://www.inf.usi.ch/phd/zambon/" class="name">Daniele Zambon</a></b></h3>
										<p>Ph.D. student (M.Sc. in Mathematics). <br> 
										His research addresses learning in non-stationary environments for graphs.</p>
									</li>
									<li>
										<span class="image"><img src="images/DG.jpg" alt="" /></span>
										<h3><b><a href="https://danielegrattarola.github.io/" class="name">Daniele Grattarola</a></b></h3>
										<p>Ph.D. student (M.Sc. in CS). <br>  
										His research focuses on graph machine learning and graph neural networks.</p>
									</li>
								</ul>
								<ul class="people">
									<li>
										<span class="image"><img src="images/PV.jpg" alt="" /></span>
										<h3><b><a href="https://search.usi.ch/en/people/7ee23dde1de92c855314b53239fdf877/verzelli-pietro" class="name">Pietro Verzelli</a></b></h3>
										<p>Ph.D. student (Master in Physics). <br> 
										He studies the dynamical properties of recurrent neural networks.</p>
									</li>
									<li>
										<span class="image"><img src="images/AC.jpg" alt="" /></span>
										<h3><b><a href="https://search.usi.ch/en/people/8bf7a374acb3ccca3ab4c316b163c1e5/cini-andrea" class="name">Andrea Cini</a></b></h3>
										<p>Ph.D. student (M.Sc. in CS). <br> 
										He is researching machine learning methods for smart power grids.</p>
									</li>
								</ul>
								<ul class="people">
									<li>
										<span class="image"><img src="images/MR.jpg" alt="" /></span>
										<h3><b><a href="https://search.usi.ch/en/people/7ee23dde1de92c855314b53239fdf877/verzelli-pietro" class="name">Matteo Riva</a></b></h3>
										<p>Scientific collaborator (M.SC. in CS). <br>
										His research focuses on deep learning methods for smart power grids.</p>
									</li>
								</ul>
								<footer class="major">
									<h2>External collaborators</h2>
									<ul class="people">
										<li>
											<span class="image"><img src="images/LL.jpg" alt="" /></span>
											<h3><b><a href="https://sites.google.com/site/lorenzlivi/home" class="name">Lorenzo Livi</a></b></h3>
											<p>Assistant professor at the University of Manitoba, Winnipeg (Canada) and University of Exeter, Exeter (United Kingdom). </p>
										</li>
										<li>
											<span class="image"><img src="images/FMB.jpg" alt="" /></span>
											<h3><b><a href="https://sites.google.com/view/filippombianchi/home" class="name">Filippo Maria Bianchi</a></b></h3>
											<p> Research scientist at NORCE The Norwegian Research Institute, Tromsø (Norway). His research covers deep learning in recurrent neural networks, graph neural networks, time series analysis, and reservoir computing.</p>
										</li>
									</ul>

									<h2>Alumni</h2>
									<ul class="alt" style="text-align: left;">
										<li>2018-2019: <b>Alberto Gasparin</b>, Politecnico di Milano, Milan (Italy), deep learning prediction and smart grids.</li>
									</ul>
								</footer>
							</section>

						<!-- Publications -->
							<section id="publications" class="main special">
								<header class="major">
									<h2>Publications</h2>
								</header>

								<ul class='def'>
									<li><a href='#gnn'>    Graph Neural Networks</a></li>
									<li><a href='#lnse'>   Learning in Non-Stationary Environments</a></li>
									<li><a href='#rl'>     Reinforcement Learning</a></li>
									<li><a href='#rnn'>    Dynamics of RNNs</a></li>
									<li><a href='#others'> Others</a></li>
								</ul>

								<ul class="alt" style="text-align: left;">
									<h2 id='gnn'>Graph Neural Networks</h2>
                                    <li>
                                        <a href="https://arxiv.org/abs/1909.03790"><b>Graph Random Neural Features for Distance-Preserving Graph Representations.</b></a><br>
                                        <span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
                                        <span class='venue'>International Conference on Machine Learning (2020)</span> <br>
                                        <a class="collapsible"></a>
                                        <div class="collapse-content">
                                           Graph Random Neural Features (GRNF) is a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks. GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network. 
                                           The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs, and that GRNF approximately preserves its metric structure.
                                        </div>
                                    </li>
                                    <li>
                                        <a href="https://arxiv.org/abs/1907.00481"><b>Spectral Clustering with Graph Neural Networks for Graph Pooling.</b></a><br>
                                        <span class='authors'>F. M. Bianchi, D. Grattarola, C. Alippi</span>
                                        <span class="venue">International Conference on Machine Learning (2020)</span><br>
                                        <a class="collapsible"></a>
                                        <div class="collapse-content">
                                           We propose a graph clustering approach that addresses some limitations of the spectral clustering algorithm. We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs. From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.
                                        </div>
                                    </li>
									<li>
										<a href="https://arxiv.org/abs/1910.11436"><b>Hierarchical Representation Learning in Graph Neural Networks with Node Decimation Pooling.</b></a><br>
										<span class='authors'>F. M. Bianchi, D. Grattarola, L. Livi, C. Alippi</span>
										<span class="venue">Preprint (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
										   We propose Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarsened versions of a graph by leveraging on its topology only. During training, the GNN learns new representations for the vertices and fits them to a pyramid of coarsened graphs, which is computed in a pre-processing step. As theoretical contributions, we first demonstrate the equivalence between the MAXCUT partition and the node decimation procedure on which NDP is based. Then, we propose a procedure to sparsify the coarsened graphs for reducing the computational complexity in the GNN; we also demonstrate that it is possible to drop many edges without significantly altering the graph spectra of coarsened graphs.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1901.01343"><b>Graph Neural Networks with Convolutional ARMA Filters.</b></a><br>
										<span class='authors'>F. M. Bianchi, D. Grattarola, L. Livi, C. Alippi</span>
										<span class="venue">Preprint (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We propose a novel graph convolutional layer based on auto-regressive moving average (ARMA) filters that, compared to the polynomial ones, provide a more flexible response thanks to a rich transfer function that accounts for the concept of state. We implement the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, is localized in the node space and can be applied to graphs with different topologies.
										</div>
									</li>

									<h2 id='lnse'>Learning in Non-Stationary Environments</h2>
                                    <li>
                                        <a href="https://arxiv.org/abs/1805.07113"><b>Change-Point Methods on a Sequence of Graphs.</b></a><br>
                                        <span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
                                        <span class='venue'>IEEE Transactions on Signal Processing (2019)</span> <br>
                                        <a class="collapsible"></a>
                                        <div class="collapse-content">
                                            Given a finite sequence of graphs, we propose a methodology to identify possible changes in stationarity in the stochastic process that generated such graphs. We consider a general family of attributed graphs for which both topology (vertices and edges) and associated attributes are allowed to change over time, without violating the stationarity hypothesis. Novel Change-Point Methods (CPMs) are proposed that map graphs onto vectors, apply a suitable statistical test in vector space and detect changes –if any– according to a user-defined confidence level; an estimate for the change point is provided as well. We ground our methods on theoretical results that show how the inference in the numerical vector space is related to the one in graph domain, and vice-versa. 
                                        </div>
                                    </li>
									<li>
										<a href="https://arxiv.org/abs/1907.09207"><b>Deep Learning for Time Series Forecasting: The Electric Load Case.</b></a><br>
										<span class='authors'>Alberto Gasparin, Slobodan Lukovic, C. Alippi</span>
										<span class="venue">Preprint (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											 We review and experimentally evaluate on two real-world datasets the most recent trends in electric load forecasting, by contrasting deep learning architectures on short term forecast (one day ahead prediction). Specifically, we focus on feed-forward and recurrent neural networks, sequence to sequence models and temporal convolutional neural networks along with architectural variants.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1903.07299"><b>Autoregressive Models for Sequences of Graphs.</b></a><br>
										<span class='authors'>D. Zambon, D. Grattarola, L. Livi, C. Alippi</span>
										<span class="venue">International Joint Conference on Neural Networks (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											This paper proposes an autoregressive (AR) model for sequences of graphs, which generalizes traditional AR models. A first novelty consists in formalizing the AR model for a very general family of graphs, characterized by a variable topology, and attributes associated with nodes and edges. A graph neural network is also proposed to learn the AR function associated with the graph-generating process, and subsequently predict the next graph in a sequence.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1805.06299"><b>Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds.</b></a><br>
										<span class='authors'>D. Grattarola, D. Zambon, L. Livi, C. Alippi</span>
										<span class="venue">IEEE Transactions on Neural Networks and Learning Systems (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We focus on the problem of detecting changes in stationarity in a stream of attributed graphs. To this end, we introduce a novel change detection framework based on neural networks and Constant-curvature manifolds (CCMs), that takes into account the non-Euclidean nature of graphs. Our contribution in this work is twofold. First, via a novel approach based on adversarial learning, we compute graph embeddings by training an autoencoder to represent graphs on CCMs. Second, we introduce two novel change detection tests operating on CCMs.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1805.01360"><b>Anomaly and Change Detection in Graph Streams through Constant-Curvature Manifold Embeddings.</b></a><br>
										<span class='authors'>D. Zambon, L. Livi, C. Alippi</span>
										<span class="venue">IEEE International Joint Conference on Neural Networks (2018)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We investigate how embedding graphs on constant-curvature manifolds (hyper-spherical and hyperbolic manifolds) impacts on the ability to detect changes in sequences of attributed graphs. The proposed methodology consists in embedding graphs into a geometric space and perform change detection there by means of conventional methods for numerical streams.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1706.06941"><b>Concept Drift and Anomaly Detection in Graph Streams.</b></a><br>
										<span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
										<span class='venue'>IEEE Transactions on Neural Networks and Learning Systems (2018)</span> <br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											 We consider stochastic processes generating graphs and propose a methodology for detecting changes in stationarity of such processes. The methodology acts by embedding every graph of the stream into a vector domain, where a conventional multivariate change detection procedure can be easily applied. We ground the soundness of our proposal by proving several theoretical results.
										</div>
									</li>
									<li>
										<a href="https://ieeexplore.ieee.org/document/8285273"><b>Detecting Changes in Sequences of Attributed Graphs.</b></a><br>
										<span class='authors'>D. Zambon, L. Livi, C. Alippi</span>
										<span class="venue">IEEE Symposium Series on Computational Intelligence (2017)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We consider a methodology for detecting changes in sequences of graphs. Changes are recognized by embedding each graph into a vector space, where conventional change detection procedures exist and can be easily applied. We introduce the methodology and focus on expanding experimental evaluations on controlled yet relevant examples involving geometric graphs and Markov chains.
										</div>
									</li>

									<h2 id='rl'>Reinforcement Learning</h2>
									<li>
										<a href="https://arxiv.org/abs/1812.04314"><b>Deep Reinforcement Learning with Weighted Q-Learning.</b></a><br>
										<span class='authors'>A. Cini, C. D'Eramo, J. Peter, C. Alippi</span>
										<span class="venue">Preprint (2020)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											 Overestimation of the maximum action-value is a well-known problem that hinders Q-Learning performance, leading to suboptimal policies and unstable learning. Among several Q-Learning variants proposed to address this issue, Weighted Q-Learning (WQL) effectively reduces the bias and shows remarkable results in stochastic environments. WQL uses a weighted sum of the estimated action-values, where the weights correspond to the probability of each action-value being the maximum; however, the computation of these probabilities is only practical in the tabular settings. In this work, we provide the methodological advances to benefit from the WQL properties in Deep Reinforcement Learning (DRL), by using neural networks with Dropout Variational Inference as an effective approximation of deep Gaussian processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated estimates of epistemic uncertainty in DRL. We show that model uncertainty in DRL can be useful not only for action selection, but also action evaluation. We analyze how the novel Weighted Deep Q-Learning algorithm reduces the bias wrt relevant baselines and provide empirical evidence of its advantages on several representative benchmarks.
										</div>
									</li>

									<h2 id='rnn'>Dynamics of RNNs</h2>
									<li>
										<a href="https://arxiv.org/abs/2003.10585"><b>Input Representation in Recurrent Neural Networks Dynamics.</b></a><br>
										<span class='authors'>P. Verzelli, C. Alippi, L. Livi, P. Tino</span>
										<span class="venue">Preprint (2020)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											Reservoir computing is a popular approach to design recurrent neural networks, due to its training simplicity and its approximation performance. The recurrent part of these networks is not trained (e.g. via gradient descent), making them appealing for analytical studies, raising the interest of a vast community of researcher spanning from dynamical systems to neuroscience. It emerges that, even in the simple linear case, the working principle of these networks is not fully understood and the applied research is usually driven by heuristics. A novel analysis of the dynamics of such networks is proposed, which allows one to express the state evolution using the controllability matrix. Such a matrix encodes salient characteristics of the network dynamics: in particular, its rank can be used as an input-indepedent measure of the memory of the network. Using the proposed approach, it is possible to compare different architectures and explain why a cyclic topology achieves favourable results.
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1903.11691"><b>Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere.</b></a><br>
										<span class='authors'>P. Verzelli, C. Alippi, L. Livi</span>
										<span class='venue'>Nature Scientific Reports (2019)</span> <br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We propose a model of echo state networks that eliminates critical dependence on hyper-parameters, resulting in networks that provably cannot enter a chaotic regime and, at the same time, denotes nonlinear behavior in phase space characterized by a large memory of past inputs, comparable to the one of linear networks. Our contribution is supported by experiments corroborating our theoretical findings, showing that the proposed model displays dynamics that are rich-enough to approximate many common nonlinear systems used for benchmarking. 
										</div>
									</li>
									<li>
										<a href="https://arxiv.org/abs/1810.01742"><b>A Characterization of the Edge of Criticality in Binary Echo State Networks.</b></a><br>
										<span class='authors'>P. Verzelli, L. Livi, C. Alippi</span>
										<span class="venue">IEEE International Workshop on Machine Learning for Signal Processing (2018)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											We propose binary echo state networks (ESNs), which are architecturally equivalent to standard ESNs but consider binary activation functions and binary recurrent weights. For these networks, we derive a closed-form expression for the edge of criticality (EoC) in the autonomous case and perform simulations in order to assess their behavior in the case of noisy neurons and in the presence of a signal. We propose a theoretical explanation for the fact that the variance of the input plays a major role in characterizing the EoC. 
										</div>
									</li>

									<h2 id='others'>Others</h2>
									<li>
										<a href="https://arxiv.org/abs/1812.04314"><b>Adversarial Autoencoders with Constant-Curvature Latent Manifolds.</b></a><br>
										<span class='authors'>D. Grattarola, L. Livi, C. Alippi</span>
										<span class="venue">Applied Soft Computing (2019)</span><br>
										<a class="collapsible"></a>
										<div class="collapse-content">
											 We introduce the constant-curvature manifold adversarial autoencoder (CCM-AAE), a probabilistic generative model trained to represent a data distribution on a constant-curvature Riemannian manifold (CCM). Our method works by matching the aggregated posterior of the CCM-AAE with a probability distribution defined on a CCM, so that the encoder implicitly learns to represent data on the CCM to fool the discriminator network. The geometric constraint is also explicitly imposed by jointly training the CCM-AAE to maximize the membership degree of the embeddings to the CCM.
										</div>
									</li>
									
									
								</ul>
							</section>

						<!-- Projects -->
							<section id="oss" class="main special">
								<header class="major">
									<h2>Open Source</h2>
								</header>
								<ul class="oss">
									<li>
										<span class="image">
											<a href="https://danielegrattarola.github.io/spektral">
												<img src="images/logo_spektral.svg" alt=""/>
											</a>
										</span>
										<h3><b><a href="https://danielegrattarola.github.io/spektral" class="name">Spektral</a></b></h3>
										<p>A library for building graph neural networks in Keras and Tensorflow.</p>
									</li>
									<li>
										<span class="image">
											<a href="https://github.com/dzambon/cdg">
												<img src="images/logo_cdg.svg" alt="" />
											</a>
										</span>
										<h3><b><a href="https://github.com/dzambon/cdg" class="name">CDG</a></b></h3>
										<p>A Python library for detecting changes in stationarity in sequences of graphs.</p>
									</li>
									<li>
										<span class="image">
											<a href="https://github.com/albertogaspar/dts">
												<img src="images/logo_dts.svg" alt="" />
											</a>
										</span>
										<h3><b><a href="https://github.com/albertogaspar/dts" class="name">DTS</a></b></h3>
										<p>A Keras library that provides multiple deep architectures for multi-step time-series forecasting.</p>
									</li>
								</ul>
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>Contacts</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>Università della Svizzera italiana</dd>
								<dd>Via Giuseppe Buffi, 13 &bull; 6900 Lugano &bull; Switzerland</dd>
								<dt>Email</dt>
								<dd><a href="#">info@gmlg.ch</a></dd>	
							</dl>
						</section>
						<section>
							<h2>Social</h2>
							<ul class="icons">
								<li><a href="https://twitter.com/GMLG_Lugano" class="icon brands fa-twitter alt"><span class="label">Twitter</span></a></li>
								<li><a href="https://github.com/GMLG-Lugano" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; Graph Machine Learning Group @ ALaRI. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/particles.min.js"></script>
			<script src="assets/js/particles-app.js"></script>
			<script src="assets/js/collapsible.js"></script>

	</body>
</html>
