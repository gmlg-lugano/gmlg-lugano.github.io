
<html>
	<head>
		<title>GMLG - Publications</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/particles.css" />
		<link rel="stylesheet" href="assets/css/collapsible.css" />
		<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125823175-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-125823175-2');
		</script>
	</head>
	<body class="is-preload">
	<div id="particles-js"></div>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<span class="logo"><img src="images/logo.svg" alt="" /></span>
			<h1>Graph Machine Learning Group</h1>
		</header>

		<!-- Nav -->
		<nav id="nav">
			<ul>
				<li><a href="/index.html">About</a></li>
				<li><a href="/people.html">People</a></li>
				<li><a href="/publications.html" class="active">Publications</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">

		<!-- Publications -->
		<section id="publications" class="main special">
			<header class="major">
				<h2>Publications</h2>
			</header>

			<ul class='def'>
				<li><a href='#2022'>2022</a></li>
				<li><a href='#2021'>2021</a></li>
				<li><a href='#2020'>2020</a></li>
				<li><a href='#2019'>2019</a></li>
				<li><a href='#2018'>2018</a></li>
				<li><a href='#2017'>2017</a></li>
			</ul>

			<ul class="alt" style="text-align: left;">
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<h2 id='2022'>2022</h2>
				<li>
					<a href="https://arxiv.org/abs/2205.13479"><b>Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations.</b></a><br>
					<span class='authors'>I. Marisca, A. Cini, C. Alippi</span>
					<span class="venue">Preprint</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Modeling multivariate time series as temporal signals over a (possibly dynamic) graph is an effective representational framework that allows for developing models for time series analysis. In fact, discrete sequences of graphs can be processed by autoregressive graph neural networks to recursively learn representations at each discrete point in time and space. Spatiotemporal graphs are often highly sparse, with time series characterized by multiple, concurrent, and even long sequences of missing data, e.g., due to the unreliable underlying sensor network. In this context, autoregressive models can be brittle and exhibit unstable learning dynamics. The objective of this paper is, then, to tackle the problem of learning effective models to reconstruct, i.e., impute, missing data points by conditioning the reconstruction only on the available observations. In particular, we propose a novel class of attention-based architectures that, given a set of highly sparse discrete observations, learn a representation for points in time and space by exploiting a spatiotemporal diffusion architecture aligned with the imputation task. Representations are trained end-to-end to reconstruct observations w.r.t. the corresponding sensor and its neighboring nodes. Compared to the state of the art, our model handles sparse data without propagating prediction errors or requiring a bidirectional model to encode forward and backward time dependencies. Empirical results on representative benchmarks show the effectiveness of the proposed method.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/2205.13492"><b>Sparse Graph Learning for Spatiotemporal Time Series.</b></a><br>
					<span class='authors'>A. Cini, D. Zambon, C. Alippi</span>
					<span class="venue">Preprint</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Outstanding achievements of graph neural networks for spatiotemporal time series prediction show that relational constraints introduce a positive inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data generating process is unavailable; the practitioner is then left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled -- yet practical -- probabilistic methods that learn the relational dependencies by modeling distributions over graphs while maximizing, at the same time, end-to-end the forecasting accuracy. Our novel graph learning approach, based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded and effective. We show that tailoring the gradient estimators to the graph learning problem allows us also for achieving state-of-the-art forecasting performance while controlling, at the same time, both the sparsity of the learned graph and the computational burden. We empirically assess the effectiveness of the proposed method on synthetic and real-world benchmarks, showing that the proposed solution can be used as a stand-alone graph identification procedure as well as a learned component of an end-to-end forecasting architecture.
					</div>
				</li>
				<li>
					<a href=""><b>Spatio-Temporal Graph Neural Networks for Aggregate Load Forecasting.</b></a><br>
					<span class='authors'>S. Eandi, A. Cini, S. Lukovic, C. Alippi</span>
					<span class="venue">To appear in IEEE International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						Accurate forecasting of electricity demand is a core component in many systems within the modern electricity infrastructure. Several approaches exist that tackle this problem by exploiting modern deep learning tools. However, most previous works focus on predicting the total load as a univariate time series forecasting task, ignoring all fine-grained information captured by the smart meters distributed across the power grid. We introduce a methodology to account for this information in the graph neural network framework. In particular, we consider spatio-temporal graphs where each node is associated with the aggregate load of a cluster of smart meters, and a global graph-level attribute indicates the total load on the grid. We propose two novel spatio-temporal graph neural network models to process this representation and take advantage of both the finer-grained information and the relationships existing between the different clusters of meters. We compare these models on a widely used, openly available, benchmark against a competitive baseline which only accounts for the total load profile. Within these settings, we show that the proposed methodology improves forecasting accuracy.
					</div>
				</li>
				<li>
					<a href=""><b>Graph iForest: Isolation of anomalous and outlier graphs.</b></a><br>
					<span class='authors'>D. Zambon, L. Livi, C. Alippi</span>
					<span class="venue">To appear in IEEE International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We present an anomaly and outlier detection method for graph data. The method relies on the consideration that anomalies and outliers are more easily isolated by certain incremental partitionings of the data space. Specifically, we build upon the isolation forest method and introduce a new incremental partitioning of the space of graphs that makes the isolation forest method applicable to generic attributed graphs, i.e., graphs where both nodes and edges can be associated with attributes. Within the considered general setup, the topology and the number of nodes can change from graph to graph, and a node correspondence between different graphs can be absent or unknown. Examples of applications of what proposed include the identification of frauds and fake news in communication networks, and breakage of systems monitored by sensor networks. The main novel contribution of the paper is a graph space partitioning which we prove to be expressive enough to identify anomalies and outlier graphs in a given dataset. 
						An empirical analysis on synthetic and real-world graphs validates the effectiveness of the proposed method.
					</div>
				</li>
				<li>
					<a href=""><b>Understanding Catastrophic Forgetting of Gated Linear Networks in Continual Learning.</b></a><br>
					<span class='authors'>M. Munari, L. Pasa, D. Zambon, C. Alippi, N. Navarin</span>
					<span class="venue">To appear in IEEE International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						In this paper, we consider the recently proposed family of continual learning models, called Gated Linear Networks (GLNs), and study two crucial aspects impacting on the amount of catastrophic forgetting affecting gated linear networks, namely, data standardization and gating mechanism.  Data standardization is particularly challenging in the online/continual learning setting because data from future tasks is not available beforehand. The results obtained using an online standardization method show a considerably higher amount of forgetting compared to an offline --static-- standardization.  Interestingly, with the latter standardization, we observe that GLNs show almost no forgetting on the considered benchmark datasets.  Secondly, for an effective GLNs, it is essential to tailor the hyperparameters of the gating mechanism to the data distribution. In this paper, we propose a gating strategy based on a set of prototypes and the resulting Voronoi tessellation. The experimental assessment shows that the proposed approach is more robust to different data standardizations compared to the original one, based on a halfspace gating mechanism, and shows improved predictive performance.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/2204.11135"><b>AZ-whiteness test: a test for uncorrelated noise on spatio-temporal graphs.</b></a><br>
					<span class='authors'>D. Zambon, C. Alippi</span>
					<span class="venue">Preprint</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We present the first whiteness test for graphs, i.e., a whiteness test for multivariate time series associated with the nodes of a dynamic graph. The statistical test aims at finding serial dependencies among close-in-time observations, as well as spatial dependencies among neighboring observations given the underlying graph. The proposed test is a spatio-temporal extension of traditional tests from the system identification literature and finds applications in similar, yet more general, application scenarios involving graph signals. The AZ-test is versatile, allowing the underlying graph to be dynamic, changing in topology and set of nodes, and weighted, thus accounting for connections of different strength, as is the case in many application scenarios like transportation networks and sensor grids. The asymptotic distribution --- as the number of graph edges or temporal observations increases --- is known, and does not assume identically distributed data. We validate the practical value of the test on both synthetic and real-world problems, and show how the test can be employed to assess the quality of spatio-temporal forecasting models by analyzing the prediction residuals appended to the graphs stream.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/2110.05292"><b>Understanding Pooling in Graph Neural Networks.</b></a><br>
					<span class='authors'>D. Grattarola, D. Zambon, F. M. Bianchi, C. Alippi</span>
					<span class="venue">IEEE Transactions on Neural Networks and Learning Systems</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Inspired by the conventional pooling layers in convolutional neural networks, many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. The great variety in the literature stems from the many possible strategies for coarsening a graph, which may depend on different assumptions on the graph structure or the specific downstream task. In this paper we propose a formal characterization of graph pooling based on three main operations, called selection, reduction, and connection, with the goal of unifying the literature under a common framework. Following this formalization, we introduce a taxonomy of pooling operators and categorize more than thirty pooling methods proposed in recent literature. We propose criteria to evaluate the performance of a pooling operator and use them to investigate and contrast the behavior of different classes of the taxonomy on a variety of tasks.
					</div>
				</li>

<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<hr>
				<h2 id='2021'>2021</h2>
				<li>
					<a href="https://proceedings.neurips.cc/paper/2021/hash/af87f7cdcda223c41c3f3ef05a3aaeea-Abstract.html"><b>Learning Graph Cellular Automata.</b></a><br>
					<span class='authors'>D. Grattarola, L. Livi, C. Alippi</span>
					<span class="venue">Neural Information Processing Systems</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. 
						In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. 
						In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. 
						First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. 
						Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.
					</div>
				</li>
				<li>
					<a href="https://jmlr.org/papers/v22/20-633.html"><b>Gaussian Approximation for Bias Reduction in Q-Learning.</b></a><br>
					<span class='authors'>C. D'Eramo, A. Cini, A. Nuara, M. Pirotta, C. Alippi, J. Peters, M. Restelli</span>
					<span class="venue">Journal of Machine Learning Research</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Temporal-Difference off-policy algorithms are among the building blocks of reinforcement learning (RL). Within this family, Q-Learning is arguably the most famous one, which has been widely studied and extended. The update rule of Q-learning involves the use of the maximum operator to estimate the maximum expected value of the return. However, this estimate is positively biased, and may hinder the learning process, especially in stochastic environments and when function approximation is used. We introduce the Weighted Estimator as an effective solution to mitigate the negative effects of overestimation in Q-Learning. The Weighted Estimator estimates the maximum expected value as a weighted sum of the action values, with the weights being the probabilities that each action value is the maximum. In this work, we study the problem from the statistical perspective of estimating the maximum expected value of a set of random variables and provide bounds to the bias and the variance of the Weighted Estimator, showing its advantages over other estimators present in literature. Then, we derive algorithms to enable the use of the Weighted Estimator, in place of the Maximum Estimator, in online and batch RL, and we introduce a novel algorithm for deep RL. Finally, we empirically evaluate our algorithms in a large set of heterogeneous problems, encompassing discrete and continuous, low and high dimensional, deterministic and stochastic environments. Experimental results show the effectiveness of the Weighted Estimator in controlling the bias of the estimate, resulting in better performance than representative baselines and robust learning w.r.t. a large set of diverse environments.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/2108.00298"><b>Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks.</b></a><br>
					<span class='authors'>A. Cini, I. Marisca, C. Alippi</span>
					<span class="venue">International Conference on Learning Representations</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.
					</div>
				</li>
				<li>
					<a href=""><b>Deep learning for graphs (special session at ESANN 2021).</b></a><br>
					<span class='authors'>D. Bacciu, F. M. Bianchi, B. Paassen, C. Alippi</span>
					<span class="venue">European Symposium on Artificial Neural Networks</span><br>
				</li>
				<li>
					<a href="https://www.ieee-jas.net/fileZDHXBEN/journal/article/zdhxbywb/newcreate/JAS-2021-0798.pdf"><b>Precise Agriculture: Effective Deep Learning Strategies to Detect Pest Insects.</b></a><br>
					<span class='authors'>L. Butera, A. Ferrante, M. Jermini, M. Prevostini, C. Alippi</span>
					<span class="venue">Journal of Automatica Sinica</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Pest  insect  monitoring  and  control  is  crucial  toensure a safe and profitable crop growth in all plantation types, aswell as guarantee food quality and limited use of pesticides. Weaim  at  extending  traditional  monitoring  by  means  of  traps,  byinvolving the general public in reporting the presence of insectsby  using  smartphones.  This  includes  the  largely  unexploredproblem  of  detecting  insects  in  images  that  are  taken  in  non-controlled  conditions.  Furthermore,  pest  insects  are,  in  manycases,  extremely  similar  to  other  species  that  are  harmless.Therefore,  computer  vision  algorithms  must  not  be  fooled  bythese  similar  insects,  not  to  raise  unmotivated  alarms.  In  thiswork,  we  study  the  capabilities  of  state-of-the-art  (SoA)  objectdetection models based on convolutional neural networks (CNN)for  the  task  of  detecting  beetle-like  pest  insects  on  non-homogeneous  images  taken  outdoors  by  different  sources.Moreover, we focus on disambiguating a pest insect from similarharmless species. We consider not only detection performance ofdifferent models, but also required computational resources. Thisstudy aims at providing a baseline model for this kind of tasks.Our results show the suitability of current SoA models for thisapplication, highlighting how FasterRCNN with a MobileNetV3backbone is a particularly good starting point for accuracy andinference execution latency. This combination provided a meanaverage  precision  score  of  92.66%  that  can  be  consideredqualitatively  at  least  as  good  as  the  score  obtained  by  otherauthors that adopted more specific models.
					</div>
				</li>
				<li>
                    <a href="https://openreview.net/forum?id=dlEJsyHGeaL"><b>Graph Edit Networks.</b></a><br>
                    <span class='authors'>B. Paassen, D. Grattarola, D.Zambon, C. Alippi, B. Hammer</span>
                    <span class='venue'>International Conference on Learning Representations</span><br>
                    <a class="collapsible"></a>
                    <div class="collapse-content">
                       While graph neural networks have made impressive progress in classification and regression on graphs, few approaches to date perform time series prediction on graphs and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular,  graph edits are general enough to describe any graph-to-graph change, not only edge changes, they are sparse, making them easier to understand for humans and more efficient computationally, and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a simple linear layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. Theoretically, we show that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept.
                    </div>
                </li>
                <li>
					<a href="https://www.biorxiv.org/content/10.1101/2020.12.03.409979v1.abstract"><b>Seizure localisation with attention-based graph neural networks.</b></a><br>
					<span class='authors'>D. Grattarola, L. Livi, C. Alippi, R. Wennberg, T. A. Valiante</span>
					<span class="venue">Preprint</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						Graph neural networks (GNNs) and the attention mechanism are two of the most significant advances in artificial intelligence methods over the past few years. The former are neural networks able to process graph-structured data, while the latter learns to selectively focus on those parts of the input that are more relevant for the task at hand. In this paper, we propose a methodology for seizure localisation which combines the two approaches.
						Our method is composed of several blocks. First, we represent brain states in a compact way by computing functional networks from intracranial electroencephalography recordings, using metrics to quantify the coupling between the activity of different brain areas. Then, we train a GNN to correctly distinguish between functional networks associated with interictal and ictal phases. The GNN is equipped with an attention-based layer which automatically learns to identify those regions of the brain (associated with individual electrodes) that are most important for a correct classification. The localisation of these regions is fully unsupervised, meaning that it does not use any prior information regarding the seizure onset zone.
						We report results both for human patients and for simulators of brain activity. We show that the regions of interest identified by the GNN strongly correlate with the localisation of the seizure onset zone reported by electroencephalographers. We also show that our GNN exhibits uncertainty on those patients for which the clinical localisation was also unsuccessful, highlighting the robustness of the proposed approach.
					</div>
				</li>
                <li>
					<a href="https://arxiv.org/abs/2010.02860"><b>Learn to Synchronize, Synchronize to Learn.</b></a><br>
					<span class='authors'>P. Verzelli, C. Alippi, L. Livi</span>
					<span class="venue">Chaos</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						In recent years, the machine learning community has seen a continuous growing interest in research aimed at investigating dynamical aspects of both training procedures and perfected models. Of particular interest among recurrent neural networks, we have the Reservoir Computing (RC) paradigm for its conceptual simplicity and fast training scheme. Yet, the guiding principles under which RC operates are only partially understood. In this work, we study the properties behind learning dynamical systems with RC and propose a new guiding principle based on Generalized Synchronization (GS) granting its feasibility. We show that the well-known Echo State Property (ESP) implies and is implied by GS, so that theoretical results derived from the ESP still hold when GS does. However, by using GS one can profitably study the RC learning procedure by linking the reservoir dynamics with the readout training. Notably, this allows us to shed light on the interplay between the input encoding performed by the reservoir and the output produced by the readout optimized for the task at hand. In addition, we show that - as opposed to the ESP - satisfaction of the GS can be measured by means of the Mutual False Nearest Neighbors index, which makes effective to practitioners theoretical derivations.
					</div>
				</li>
                <li>
					<a href="https://arxiv.org/abs/2003.10585"><b>Input-to-State Representation in Linear Reservoir Dynamics.</b></a><br>
					<span class='authors'>P. Verzelli, C. Alippi, L. Livi, P. Tino</span>
					<span class="venue"> IEEE Transactions on Neural Networks and Learning Systems</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						Reservoir computing is a popular approach to design recurrent neural networks, due to its training simplicity and its approximation performance. The recurrent part of these networks is not trained (e.g. via gradient descent), making them appealing for analytical studies, raising the interest of a vast community of researcher spanning from dynamical systems to neuroscience. It emerges that, even in the simple linear case, the working principle of these networks is not fully understood and the applied research is usually driven by heuristics. A novel analysis of the dynamics of such networks is proposed, which allows one to express the state evolution using the controllability matrix. Such a matrix encodes salient characteristics of the network dynamics: in particular, its rank can be used as an input-indepedent measure of the memory of the network. Using the proposed approach, it is possible to compare different architectures and explain why a cyclic topology achieves favourable results.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1907.09207"><b>Deep Learning for Time Series Forecasting: The Electric Load Case.</b></a><br>
					<span class='authors'>A. Gasparin, S. Lukovic, C. Alippi</span>
					<span class="venue">CAAI Transactions on Intelligence Technology</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Management and efficient operations in critical infrastructure such as Smart Grids take huge advantage of accurate power load forecasting which, due to its nonlinear nature, remains a challenging task. Recently, deep learning has emerged in the machine learning field achieving impressive performance in a vast range of tasks, from image classification to machine translation. Applications of deep learning models to the electric load forecasting problem are gaining interest among researchers as well as the industry, but a comprehensive and sound comparison among different architectures is not yet available in the literature. This work aims at filling the gap by reviewing and experimentally evaluating on two real-world datasets the most recent trends in electric load forecasting, by contrasting deep learning architectures on short term forecast (one day ahead prediction). Specifically, we focus on feedforward and recurrent neural networks, sequence to sequence models and temporal convolutional neural networks along with architectural variants, which are known in the signal processing community but are novel to the load forecasting one.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1901.01343"><b>Graph Neural Networks with Convolutional ARMA Filters.</b></a><br>
					<span class='authors'>F. M. Bianchi, D. Grattarola, L. Livi, C. Alippi</span>
					<span class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We propose a novel graph convolutional layer based on auto-regressive moving average (ARMA) filters that, compared to the polynomial ones, provide a more flexible response thanks to a rich transfer function that accounts for the concept of state. We implement the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, is localized in the node space and can be applied to graphs with different topologies.
					</div>
				</li>

<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<hr>
				<h2 id='2020'>2020</h2>
				<li>
					<a href="https://arxiv.org/abs/2003.09280"><b>Deep Reinforcement Learning with Weighted Q-Learning.</b></a><br>
					<span class='authors'>A. Cini, C. D'Eramo, J. Peters, C. Alippi</span>
					<span class="venue">Preprint</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Overestimation of the maximum action-value is a well-known problem that hinders Q-Learning performance, leading to suboptimal policies and unstable learning. Among several Q-Learning variants proposed to address this issue, Weighted Q-Learning (WQL) effectively reduces the bias and shows remarkable results in stochastic environments. WQL uses a weighted sum of the estimated action-values, where the weights correspond to the probability of each action-value being the maximum; however, the computation of these probabilities is only practical in the tabular settings. In this work, we provide the methodological advances to benefit from the WQL properties in Deep Reinforcement Learning (DRL), by using neural networks with Dropout Variational Inference as an effective approximation of deep Gaussian processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated estimates of epistemic uncertainty in DRL. We show that model uncertainty in DRL can be useful not only for action selection, but also action evaluation. We analyze how the novel Weighted Deep Q-Learning algorithm reduces the bias wrt relevant baselines and provide empirical evidence of its advantages on several representative benchmarks.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1910.11436"><b>Hierarchical Representation Learning in Graph Neural Networks with Node Decimation Pooling.</b></a><br>
					<span class='authors'>F. M. Bianchi, D. Grattarola, L. Livi, C. Alippi</span>
					<span class="venue">IEEE Transactions on Neural Networks and Learning Systems</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
					   We propose Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarsened versions of a graph by leveraging on its topology only. During training, the GNN learns new representations for the vertices and fits them to a pyramid of coarsened graphs, which is computed in a pre-processing step. As theoretical contributions, we first demonstrate the equivalence between the MAXCUT partition and the node decimation procedure on which NDP is based. Then, we propose a procedure to sparsify the coarsened graphs for reducing the computational complexity in the GNN; we also demonstrate that it is possible to drop many edges without significantly altering the graph spectra of coarsened graphs.
					</div>
				</li>
				<li>
                    <a href="https://arxiv.org/abs/1909.03790"><b>Graph Random Neural Features for Distance-Preserving Graph Representations.</b></a><br>
                    <span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
                    <span class='venue'>International Conference on Machine Learning</span><br>
                    <a class="collapsible"></a>
                    <div class="collapse-content">
                       Graph Random Neural Features (GRNF) is a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks. GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network. 
                       The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs, and that GRNF approximately preserves its metric structure.
                    </div>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/1907.00481"><b>Spectral Clustering with Graph Neural Networks for Graph Pooling.</b></a><br>
                    <span class='authors'>F. M. Bianchi, D. Grattarola, C. Alippi</span>
                    <span class="venue">International Conference on Machine Learning</span><br>
                    <a class="collapsible"></a>
                    <div class="collapse-content">
                       We propose a graph clustering approach that addresses some limitations of the spectral clustering algorithm. We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs. From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.
                    </div>
                </li>
                <li>
					<a href="https://ieeexplore.ieee.org/document/9207503"><b>Cluster-based Aggregate Load Forecasting with Deep Neural Networks.</b></a><br>
					<span class='authors'>A. Cini, S. Lukovic, C. Alippi</span>
					<span class="venue"> International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 Highly accurate power demand forecasting represents one of key challenges of Smart Grid applications. In this setting, a large number of Smart Meters produces huge amounts of data that need to be processed to predict the load requested by the grid. Due to the high dimensionality of the problem, this often results in the adoption of simple aggregation strategies for the power that fail in capturing the relational information existing among the different types of user. A possible alternative, known as Cluster-based Aggregate Forecasting, consists in clustering the load profiles and, on top of that, building predictors of the aggregate at the cluster-level. In this work we explore the technique in the context of predictors based on deep recurrent neural networks and address the scalability issues presenting neural architectures adequate to process cluster-level aggregates. The proposed methods are finally evaluated both on a publicly available benchmark and a heterogenous dataset of Smart Meter data from an entire, medium-sized, Swiss town.
					</div>
				</li>

<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<hr>
				<h2 id='2019'>2019</h2>
				<li>
					<a href="https://arxiv.org/abs/1907.09207"><b>Deep Learning for Time Series Forecasting: The Electric Load Case.</b></a><br>
					<span class='authors'>Alberto Gasparin, Slobodan Lukovic, C. Alippi</span>
					<span class="venue">Preprint (2019)</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 We review and experimentally evaluate on two real-world datasets the most recent trends in electric load forecasting, by contrasting deep learning architectures on short term forecast (one day ahead prediction). Specifically, we focus on feed-forward and recurrent neural networks, sequence to sequence models and temporal convolutional neural networks along with architectural variants.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1903.11691"><b>Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere.</b></a><br>
					<span class='authors'>P. Verzelli, C. Alippi, L. Livi</span>
					<span class='venue'>Nature Scientific Reports</span> <br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We propose a model of echo state networks that eliminates critical dependence on hyper-parameters, resulting in networks that provably cannot enter a chaotic regime and, at the same time, denotes nonlinear behavior in phase space characterized by a large memory of past inputs, comparable to the one of linear networks. Our contribution is supported by experiments corroborating our theoretical findings, showing that the proposed model displays dynamics that are rich-enough to approximate many common nonlinear systems used for benchmarking. 
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1903.07299"><b>Autoregressive Models for Sequences of Graphs.</b></a><br>
					<span class='authors'>D. Zambon, D. Grattarola, L. Livi, C. Alippi</span>
					<span class="venue">International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						This paper proposes an autoregressive (AR) model for sequences of graphs, which generalizes traditional AR models. A first novelty consists in formalizing the AR model for a very general family of graphs, characterized by a variable topology, and attributes associated with nodes and edges. A graph neural network is also proposed to learn the AR function associated with the graph-generating process, and subsequently predict the next graph in a sequence.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1812.04314"><b>Adversarial Autoencoders with Constant-Curvature Latent Manifolds.</b></a><br>
					<span class='authors'>D. Grattarola, L. Livi, C. Alippi</span>
					<span class="venue">Applied Soft Computing</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 We introduce the constant-curvature manifold adversarial autoencoder (CCM-AAE), a probabilistic generative model trained to represent a data distribution on a constant-curvature Riemannian manifold (CCM). Our method works by matching the aggregated posterior of the CCM-AAE with a probability distribution defined on a CCM, so that the encoder implicitly learns to represent data on the CCM to fool the discriminator network. The geometric constraint is also explicitly imposed by jointly training the CCM-AAE to maximize the membership degree of the embeddings to the CCM.
					</div>
				</li>
				<li>
                    <a href="https://arxiv.org/abs/1805.07113"><b>Change-Point Methods on a Sequence of Graphs.</b></a><br>
                    <span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
                    <span class='venue'>IEEE Transactions on Signal Processing</span> <br>
                    <a class="collapsible"></a>
                    <div class="collapse-content">
                        Given a finite sequence of graphs, we propose a methodology to identify possible changes in stationarity in the stochastic process that generated such graphs. We consider a general family of attributed graphs for which both topology (vertices and edges) and associated attributes are allowed to change over time, without violating the stationarity hypothesis. Novel Change-Point Methods (CPMs) are proposed that map graphs onto vectors, apply a suitable statistical test in vector space and detect changes –if any– according to a user-defined confidence level; an estimate for the change point is provided as well. We ground our methods on theoretical results that show how the inference in the numerical vector space is related to the one in graph domain, and vice-versa. 
                    </div>
                </li>
				<li>
					<a href="https://arxiv.org/abs/1805.06299"><b>Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature Manifolds.</b></a><br>
					<span class='authors'>D. Grattarola, D. Zambon, L. Livi, C. Alippi</span>
					<span class="venue">IEEE Transactions on Neural Networks and Learning Systems</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We focus on the problem of detecting changes in stationarity in a stream of attributed graphs. To this end, we introduce a novel change detection framework based on neural networks and Constant-curvature manifolds (CCMs), that takes into account the non-Euclidean nature of graphs. Our contribution in this work is twofold. First, via a novel approach based on adversarial learning, we compute graph embeddings by training an autoencoder to represent graphs on CCMs. Second, we introduce two novel change detection tests operating on CCMs.
					</div>
				</li>

<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<hr>
				<h2 id='2018'>2018</h2>
				<li>
					<a href="https://arxiv.org/abs/1810.01742"><b>A Characterization of the Edge of Criticality in Binary Echo State Networks.</b></a><br>
					<span class='authors'>P. Verzelli, L. Livi, C. Alippi</span>
					<span class="venue">IEEE International Workshop on Machine Learning for Signal Processing</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We propose binary echo state networks (ESNs), which are architecturally equivalent to standard ESNs but consider binary activation functions and binary recurrent weights. For these networks, we derive a closed-form expression for the edge of criticality (EoC) in the autonomous case and perform simulations in order to assess their behavior in the case of noisy neurons and in the presence of a signal. We propose a theoretical explanation for the fact that the variance of the input plays a major role in characterizing the EoC. 
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1805.01360"><b>Anomaly and Change Detection in Graph Streams through Constant-Curvature Manifold Embeddings.</b></a><br>
					<span class='authors'>D. Zambon, L. Livi, C. Alippi</span>
					<span class="venue">IEEE International Joint Conference on Neural Networks</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We investigate how embedding graphs on constant-curvature manifolds (hyper-spherical and hyperbolic manifolds) impacts on the ability to detect changes in sequences of attributed graphs. The proposed methodology consists in embedding graphs into a geometric space and perform change detection there by means of conventional methods for numerical streams.
					</div>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1706.06941"><b>Concept Drift and Anomaly Detection in Graph Streams.</b></a><br>
					<span class='authors'>D. Zambon, C. Alippi, L. Livi</span>
					<span class='venue'>IEEE Transactions on Neural Networks and Learning Systems</span> <br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						 We consider stochastic processes generating graphs and propose a methodology for detecting changes in stationarity of such processes. The methodology acts by embedding every graph of the stream into a vector domain, where a conventional multivariate change detection procedure can be easily applied. We ground the soundness of our proposal by proving several theoretical results.
					</div>
				</li>

<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<!----------------------------------------------------------------------------->
				<hr>
				<h2 id='2017'>2017</h2>
				<li>
					<a href="https://ieeexplore.ieee.org/document/8285273"><b>Detecting Changes in Sequences of Attributed Graphs.</b></a><br>
					<span class='authors'>D. Zambon, L. Livi, C. Alippi</span>
					<span class="venue">IEEE Symposium Series on Computational Intelligence</span><br>
					<a class="collapsible"></a>
					<div class="collapse-content">
						We consider a methodology for detecting changes in sequences of graphs. Changes are recognized by embedding each graph into a vector space, where conventional change detection procedures exist and can be easily applied. We introduce the methodology and focus on expanding experimental evaluations on controlled yet relevant examples involving geometric graphs and Markov chains.
					</div>
				</li>
			</ul>
		</section>
		</div>

		<!-- Footer -->
		<footer id="footer">
			<section>
				<h2>Contacts</h2>
				<dl class="alt">
					<dt>Address</dt>
					<dd>Università della Svizzera italiana</dd>
					<dd>Via Giuseppe Buffi, 13 &bull; 6900 Lugano &bull; Switzerland</dd>
					<dt>Email</dt>
					<dd><a href="#">info@gmlg.ch</a></dd>	
				</dl>
			</section>
			<section>
				<h2>Social</h2>
				<ul class="icons">
					<li><a href="https://twitter.com/GMLG_Lugano" class="icon brands fa-twitter alt"><span class="label">Twitter</span></a></li>
                    <li><a href="https://www.youtube.com/channel/UCbdL8uWxPCRzV3HYQhMKGuA" class="icon brands fa-youtube alt"><span class="label">Twitter</span></a></li>
					<li><a href="https://github.com/GMLG-Lugano" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
				</ul>
			</section>
			<p class="copyright">&copy; Graph Machine Learning Group @ IDSIA. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
		</footer>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
	<script src="assets/js/particles.min.js"></script>
	<script src="assets/js/particles-app.js"></script>
	<script src="assets/js/collapsible.js"></script>

	</body>
</html>
